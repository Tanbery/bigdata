version: '3' 
services:

  #namenode:
  #  image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
  #  container_name: namenode
  #  volumes:
  #    - /tmp/hdfs/namenode:/hadoop/dfs/name
  #  environment:
  #    - CLUSTER_NAME=test
  #  env_file:
  #    - ./hadoop.env
  #  ports:
  #    - "50070:50070"
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.2
  
  #datanode:
  #  image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
  #  container_name: datanode
  #  volumes:
  #    - /tmp/hdfs/datanode:/hadoop/dfs/data
  #  env_file:
  #    - ./hadoop.env
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:50070"
  #  depends_on:
  #    - namenode
  #  ports:
  #    - "50075:50075"
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.3

  #resourcemanager:
  #  image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
  #  container_name: resourcemanager
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
  #  env_file:
  #    - ./hadoop.env
  #  ports:
  #    - 8088:8088
  #  networks:
  #      net_pet:
  #        ipv4_address: 172.27.1.4
  
  #nodemanager:
  #  image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
  #  container_name: nodemanager
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
  #  env_file:
  #    - ./hadoop.env
  #  ports:
  #    - 8042:8042
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.5
  
  #zookeeper:
  #  image: wurstmeister/zookeeper:3.4.6
  #  ports:
  #    - "2181:2181"
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.6

  #hive-server:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-server
  #  env_file:
  #    - ./hadoop.env
  #  environment:
  #    SERVICE_PRECONDITION: "hive-metastore:9083"
  #  ports:
  #    - "10000:10000"
  #  depends_on:
  #    - hive-metastore
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.7
  
  #hive-metastore:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-metastore
  #  env_file:
  #    - ./hadoop.env
  #  command: /opt/hive/bin/hive --service metastore
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
  #  ports:
  #    - "9083:9083"
  #  depends_on:
  #    - hive-metastore-postgresql
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.8
  
  #hive-metastore-postgresql:
  #  image: bde2020/hive-metastore-postgresql:2.3.0
  #  container_name: hive-metastore-postgresql
  #  depends_on:
  #    - datanode
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.9
    
  spark-master:
    image: bde2020/spark-master:2.4.0-hadoop2.7
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
      - 4040:4040

    env_file:
      - ./hadoop.env
    networks:
      net_pet:
        ipv4_address: 172.27.1.10
     
  spark-worker:
    image: bde2020/spark-worker:2.4.0-hadoop2.7
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"
      #- 4040:4040
    env_file:
      - ./hadoop.env
    networks:
      net_pet:
        ipv4_address: 172.27.1.11

  # jobmanager:
  #   image: flink:latest
  #   container_name: jobmanager
  #   ports:
  #     - "8081:8081"
  #   # command: standalone-job --job-classname com.job.ClassName [--job-id <job id>] [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] [job arguments]
  #   # command: docker-entrypoint.sh jobmanager
  #   volumes:
  #     - /host/path/to/job/artifacts:/opt/flink/usrlib
  #   environment:
  #     - DISABLE_JEMALLOC=true
  #     # - |
  #     #   FLINK_PROPERTIES=
  #     #   jobmanager.rpc.address: jobmanager
  #     #   parallelism.default: 2
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.10

  # taskmanager:
  #   image: flink:latest
  #   container_name: taskmanager
  #   depends_on:
  #     - jobmanager
  #   command: taskmanager
  #   scale: 1
  #   volumes:
  #     - /host/path/to/job/artifacts:/opt/flink/usrlib
  #   environment:
  #     - |
  #       FLINK_PROPERTIES=
  #       jobmanager.rpc.address: jobmanager
  #       taskmanager.numberOfTaskSlots: 2
  #       parallelism.default: 2
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.11

  zeppelin:
    image: openkbs/docker-spark-bde2020-zeppelin
    container_name: zeppelin
    restart: always
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      MASTER: "spark://spark-master:7077"
      SPARK_MASTER_URL: spark://spark-master:7077
      ZEPPELIN_PORT: 8085
      ZEPPELIN_JAVA_OPTS:
        -Dspark.driver.memory=1g
        -Dspark.executor.memory=2g
    ports:
      - "19090:8085"
    env_file:
      - ./hadoop.env
    volumes:
      - /tmp/simple-demo/zeppelin/data:/usr/lib/zeppelin/data:rw
      - /tmp/simple-demo/zeppelin/notebook:/usr/lib/zeppelin/notebook:rw
    command: /usr/lib/zeppelin/bin/zeppelin.sh
    networks:
      net_pet:
        ipv4_address: 172.27.1.12
  
  hue:
    image: gethue/hue:20191107-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
    - "8888:8888"
    volumes:
      - ./hue-overrides.ini:/usr/share/hue/desktop/conf/z-hue.ini
    depends_on:
    - database
    networks:
      net_pet:
        ipv4_address: 172.27.1.13

  database:
      image: mysql:5.7
      container_name: database
      ports:
          - "33061:3306"
      command: --init-file /data/application/init.sql
      volumes:
          - /tmp/mysql/data:/var/lib/mysql
          - ./init.sql:/data/application/init.sql
      environment:
          MYSQL_ROOT_USER: root
          MYSQL_ROOT_PASSWORD: secret
          MYSQL_DATABASE: hue
          #MYSQL_USER: root
          #MYSQL_PASSWORD: secret
      networks:
        net_pet:
          ipv4_address: 172.27.1.14


  kafka:
    image: wurstmeister/kafka:2.12-2.3.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 172.27.1.16
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    networks:
      net_pet:
        ipv4_address: 172.27.1.16  


  hbase-master:
    image: bde2020/hbase-master:1.0.0-hbase1.2.6
    container_name: hbase-master
    hostname: hbase-master
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 zoo:2181"
    ports:
      - 16010:16010
    networks:
      net_pet:
        ipv4_address: 172.27.1.20  

  hbase-region:
    image: bde2020/hbase-regionserver:1.0.0-hbase1.2.6
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    env_file:
      - ./hadoop.env
    environment:
      HBASE_CONF_hbase_regionserver_hostname: hbase-region
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 zookeeper:2181 hbase-master:16010"
    ports:
      - 16030:16030
    networks:
      net_pet:
        ipv4_address: 172.27.1.21 
  #streamsets:
  #  image: streamsets/datacollector:3.13.0-latest
  #  ports:
  #    - "18630:18630"
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.17  

networks:
  net_pet:
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/16

# if docker-compose up gives port error 
# net stop winnat
# docker-compose up ...
# net start winnat


# Verify Namenode     : http://localhost:50070/
# Verify SparkMaster  : http://localhost:8080/
# Verify Hue              : http://localhost:8888/ 